{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd8a351-d9db-443a-b6f2-6436778512ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time as t\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from datetime import *\n",
    "from collections import Counter\n",
    "import json\n",
    "from scipy.stats import percentileofscore, rankdata\n",
    "import itertools\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import csr_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e960b11-0eb8-4500-b043-799beacec51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_g_k_one(data,k=10,u_name='user_id',i_name='business_id',y_name='stars'):\n",
    "    item_group = data.groupby(i_name).agg({y_name:'count'})\n",
    "    item_g10 = item_group[item_group[y_name]>=k].index\n",
    "    data_new = data[data[i_name].isin(item_g10)]\n",
    "    user_group = data_new.groupby(u_name).agg({y_name:'count'})\n",
    "    user_g10 = user_group[user_group[y_name]>=k].index\n",
    "    data_new = data_new[data_new[u_name].isin(user_g10)]\n",
    "    return data_new\n",
    "\n",
    "def filter_tot(data,k=10,u_name='user_id',i_name='business_id',y_name='stars'):\n",
    "    data_new=data\n",
    "    while True:\n",
    "        data_new = filter_g_k_one(data_new,k=k,u_name=u_name,i_name=i_name,y_name=y_name)\n",
    "        m1 = data_new.groupby(i_name).agg({y_name:'count'})\n",
    "        m2 = data_new.groupby(u_name).agg({y_name:'count'})\n",
    "        num1 = m1[y_name].min()\n",
    "        num2 = m2[y_name].min()\n",
    "        print('item min:',num1,'user min:',num2)\n",
    "        if num1>=k and num2>=k:\n",
    "            break\n",
    "    return data_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f9c66b-da39-45b4-95aa-d203eec781c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pop_embed(perc):\n",
    "    if perc == 0:\n",
    "        return [0]*11\n",
    "    loc = int(perc//10)\n",
    "    if perc % 10 == 0:\n",
    "        return [0]*loc + [1] + [0]*(10 - loc)\n",
    "    return [0]*loc + [1 - (perc%10) / 10] + [(perc%10) / 10] + [0]*(9 - loc)\n",
    "\n",
    "def pop_embed2(perc):\n",
    "    if perc == 0:\n",
    "        return [0]*6\n",
    "    loc = int(perc//20)\n",
    "    if perc % 20 == 0:\n",
    "        return [0]*loc + [1] + [0]*(5 - loc)\n",
    "    return [0]*loc + [1 - (perc%20) / 20] + [(perc%20) / 20] + [0]*(4 - loc)\n",
    "\n",
    "def position_encoding(perc):\n",
    "    position_enc = np.array([perc / np.power(10000, 2 * (j // 2) / 10) for j in range(10)])\n",
    "    position_enc[0::2] = np.sin(position_enc[0::2]) # dim 2i\n",
    "    position_enc[1::2] = np.cos(position_enc[1::2]) # dim 2i+1\n",
    "    return position_enc\n",
    "\n",
    "basis_setup = np.insert(np.repeat(np.arange(1,6), 2),0,0)/100\n",
    "basis_setup2 = np.insert(np.repeat(np.arange(1,4), 2),0,0)/100\n",
    "\n",
    "def position_encoding_basis(perc):\n",
    "    position_enc = perc*basis_setup\n",
    "    position_enc[0::2] = np.sin(position_enc[0::2])\n",
    "    position_enc[1::2] = np.cos(position_enc[1::2])\n",
    "    return position_enc\n",
    "\n",
    "def position_encoding_basis2(perc):\n",
    "    position_enc = perc*basis_setup2\n",
    "    position_enc[0::2] = np.sin(position_enc[0::2])\n",
    "    position_enc[1::2] = np.cos(position_enc[1::2])\n",
    "    return position_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360b6d6d-0410-40a7-af54-d17534cb56f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b7f65d5-acdc-44ae-b9bc-8b11ac7e3666",
   "metadata": {},
   "source": [
    "**The following code provides the preprocessing used to construct popularity features. Variable dataset can be replaced by path to any dataset to construct corresponding features for it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebfe51c-5cb4-4921-bec7-6193f425f69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = './amazon/amazon_office'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76501644-61a5-453e-a3b9-69478eddfd03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# must have header, each row must have item, user, interaction/rating, time (as unix timestamp) in that order\n",
    "ao = pd.read_csv(f'{dataset}.csv')\n",
    "ao.columns=[\"item\", \"user\", \"rate\", \"time\"]\n",
    "ao = ao.drop_duplicates(['item', 'user'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5e4806-209a-4adb-9e63-2d06881c70c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-core filtering\n",
    "ao = filter_tot(ao,k=5,u_name='user',i_name='item',y_name='rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6314ac97-5d92-4dda-8406-0c39e0927644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user, item ids\n",
    "item_map = dict(zip(sorted(ao.item.unique()), range(len(ao.item.unique()))))\n",
    "ao.item = ao.item.apply(lambda x: item_map[x])\n",
    "user_map = dict(zip(sorted(ao.user.unique()), range(len(ao.user.unique()))))\n",
    "ao.user = ao.user.apply(lambda x: user_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2858ab2-5d2d-4f89-ac32-5be4a800f6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# month and week ids, this can be changed based on dataset\n",
    "ao['time2'] = ao.time.apply(lambda x: datetime.fromtimestamp(x))\n",
    "ao['time3'] = ao.time2.dt.year*10000 + ao.time2.dt.month*100\n",
    "var_map = dict(zip(sorted(ao['time3'].unique()), range(len(ao['time3'].unique()))))\n",
    "ao['time4'] = ao['time3'].apply(lambda x: var_map[x])\n",
    "ao['time5'] = ao.time2.dt.year*10000 + ao.time2.dt.month*100 + ao.time2.dt.isocalendar().week\n",
    "var_map = dict(zip(sorted(ao['time5'].unique()), range(len(ao['time5'].unique()))))\n",
    "ao['time6'] = ao['time5'].apply(lambda x: var_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b00d427-7f79-407c-b08e-0ace303ab6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interaction matrix processed by model\n",
    "ao.sort_values(['time2'])[['user', 'item', 'time4', 'time6']].drop_duplicates().to_csv(f'{dataset}_int2.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ea52fa-7031-4239-b79e-d72e73b8f800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 potential ways to compute popularity over time: just current period, cumulative over periods, exponential weighted average over periods\n",
    "items = sorted(ao.item.unique())\n",
    "grouped = ao.groupby('time4')\n",
    "ototaldft = pd.DataFrame(columns=[\"time4\", \"item\", \"perc\"])\n",
    "for i, ints in grouped:\n",
    "    counter = Counter(ints.item)\n",
    "    vals = list(counter.values())\n",
    "    percs = 100 * rankdata(vals, \"average\") / len(vals)\n",
    "    item_orders = list(counter.keys())\n",
    "    left = list(set(items) - set(item_orders))\n",
    "    df = pd.DataFrame({\"time4\": [i for _ in range(len(items))], \"item\": item_orders + left, \"perc\": np.concatenate((percs, np.zeros(len(left))))})\n",
    "    ototaldft = pd.concat([ototaldft, df])\n",
    "    \n",
    "ototaldft2 = pd.DataFrame(columns=[\"time4\", \"item\", \"perc\"])\n",
    "counter = Counter()\n",
    "for i, ints in grouped:\n",
    "    counter.update(ints.item)\n",
    "    vals = list(counter.values())\n",
    "    percs = 100 * rankdata(vals, \"average\") / len(vals)\n",
    "    item_orders = list(counter.keys())\n",
    "    left = list(set(items) - set(item_orders))\n",
    "    df = pd.DataFrame({\"time4\": [i for _ in range(len(items))], \"item\": item_orders + left, \"perc\": np.concatenate((percs, np.zeros(len(left))))})\n",
    "    ototaldft2 = pd.concat([ototaldft2, df])\n",
    "    \n",
    "ototaldft3 = pd.DataFrame(columns=[\"time4\", \"item\", \"perc\"])\n",
    "counter = Counter()\n",
    "for i, ints in grouped:\n",
    "    counter = Counter({k:0.5*v for k,v in counter.items()})\n",
    "    counter.update(ints.item)\n",
    "    vals = list(counter.values())\n",
    "    percs = 100 * rankdata(vals, \"average\") / len(vals)\n",
    "    item_orders = list(counter.keys())\n",
    "    left = list(set(items) - set(item_orders))\n",
    "    df = pd.DataFrame({\"time4\": [i for _ in range(len(items))], \"item\": item_orders + left, \"perc\": np.concatenate((percs, np.zeros(len(left))))})\n",
    "    ototaldft3 = pd.concat([ototaldft3, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3d8baa-e244-4fd5-958a-2f2ed49e2e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ototaldft.to_csv(f\"{dataset}_currpop.csv\", index=False)\n",
    "ototaldft2.to_csv(f\"{dataset}_cumpop.csv\", index=False)\n",
    "ototaldft3.to_csv(f\"{dataset}_wtpop.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13253e7-26b9-469b-ab41-b04ce4f3c9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(f\"{dataset}_currpop.txt\", ototaldft)\n",
    "np.savetxt(f\"{dataset}_cumpop.txt\", ototaldft2)\n",
    "np.savetxt(f\"{dataset}_wtpop.txt\", ototaldft3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80259223-195a-4561-b862-72f06430c395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct simple popularity feature based on each of 3 methods\n",
    "otmp = ototaldft.pivot(index = 'time4', columns = 'item', values='perc')\n",
    "otmp_ = otmp.apply(lambda x: list(itertools.chain.from_iterable([pop_embed(p) for p in x])))\n",
    "otmp2 = ototaldft2.pivot(index = 'time4', columns = 'item', values='perc')\n",
    "otmp2_ = otmp2.apply(lambda x: list(itertools.chain.from_iterable([pop_embed(p) for p in x])))\n",
    "otmp3 = ototaldft3.pivot(index = 'time4', columns = 'item', values='perc')\n",
    "otmp3_ = otmp3.apply(lambda x: list(itertools.chain.from_iterable([pop_embed(p) for p in x])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309c7cea-f13c-4210-af68-ffaab8b3defd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(f\"{dataset}_currembed.txt\", otmp_.values)\n",
    "np.savetxt(f\"{dataset}_cumembed.txt\", otmp2_.values)\n",
    "np.savetxt(f\"{dataset}_wtembed.txt\", otmp3_.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af664319-d9ad-4ca7-84af-8d0b01dda849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sinusoidal popularity features, only applied for weighted average\n",
    "otmp3_p = otmp3.apply(lambda x: list(itertools.chain.from_iterable([position_encoding(p) for p in x])))\n",
    "np.savetxt(f\"{dataset}_wtembed_pos.txt\", otmp3_p.values)\n",
    "otmp3_pb = otmp3.apply(lambda x: list(itertools.chain.from_iterable([position_encoding_basis(p) for p in x])))\n",
    "np.savetxt(f\"{dataset}_wtembed_pos2.txt\", otmp3_pb.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9fc52e-9a77-4775-95d4-a1cfb5c79a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture previous 4 weeks popularity (if we're at January 30th don't want to lose January 1-January 28 data)\n",
    "ototaldftw = pd.DataFrame(columns=[\"time6\", \"item\", \"perc\"])\n",
    "grouped = ao.groupby('time6')\n",
    "counter = Counter()\n",
    "for i, ints in grouped:\n",
    "    if i >= 4:\n",
    "        counter.subtract(prev4)\n",
    "    counter.update(ints.item)\n",
    "    vals = list(counter.values())\n",
    "    percs = 100 * rankdata(vals, \"average\") / len(vals)\n",
    "    item_orders = list(counter.keys())\n",
    "    left = list(set(items) - set(item_orders))\n",
    "    df = pd.DataFrame({\"time6\": [i for _ in range(len(items))], \"item\": item_orders + left, \"perc\": np.concatenate((percs, np.zeros(len(left))))})\n",
    "    ototaldftw = pd.concat([ototaldftw, df])\n",
    "    if i >= 3:\n",
    "        prev4 = prev3\n",
    "    if i >= 2:\n",
    "        prev3 = prev2\n",
    "    if i >= 1:\n",
    "        prev2 = prev1\n",
    "    prev1 = ints.item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606b7fb4-b1d2-4ea4-9fe8-adb348541348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple popularity feature w/ lower dimension to reduce time/space\n",
    "otmpw = ototaldftw.pivot(index = 'time6', columns = 'item', values='perc')\n",
    "otmpw_ = otmpw.apply(lambda x: list(itertools.chain.from_iterable([pop_embed2(p) for p in x])))\n",
    "np.savetxt(f\"{dataset}_week_embed2.txt\", otmpw_.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f11530-0e0e-44de-aa98-9edbddc8f2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sinusoidal popularity features (w/ lower dimension for 2nd)\n",
    "otmpw_p = otmpw.apply(lambda x: list(itertools.chain.from_iterable([position_encoding(p) for p in x])))\n",
    "np.savetxt(f\"{dataset}_weekembed_pos.txt\", otmpw_p.values)\n",
    "otmpw_pb = otmpw.apply(lambda x: list(itertools.chain.from_iterable([position_encoding_basis2(p) for p in x])))\n",
    "np.savetxt(f\"{dataset}_weekembed_pos2.txt\", otmpw_pb.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee3bdd1-ea14-44f1-bfcf-528d897f3178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user activity features used for reg loss\n",
    "users = sorted(ao.user.unique())\n",
    "grouped = ao[['user', 'item', 'time4']].groupby(['time4'])\n",
    "\n",
    "ototaldftd = pd.DataFrame(columns=[\"time4\", \"user\", \"perc\"])\n",
    "counter = Counter()\n",
    "for i, ints in grouped:\n",
    "    counter.update(ints.user)\n",
    "    vals = list(counter.values())\n",
    "    percs = 100 * rankdata(vals, \"average\") / len(vals)\n",
    "    user_orders = list(counter.keys())\n",
    "    left = list(set(users) - set(user_orders))\n",
    "    df = pd.DataFrame({\"time4\": [i for _ in range(len(users))], \"user\": user_orders + left, \"perc\": np.concatenate((percs, np.zeros(len(left))))})\n",
    "    ototaldftd = pd.concat([ototaldftd, df])\n",
    "\n",
    "otmpd = ototaldftd.pivot(index = 'time4', columns = 'user', values='perc')\n",
    "np.savetxt(f\"{dataset}_userhist.txt\", otmpd.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e1bebb-2e5f-4074-a975-fc6ac094c1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next 2 cells are for item-cooccurrence based feature\n",
    "# get count for all consecutive item-cooccurrences (wrt user, symmetric between before and after)\n",
    "ints = pd.read_csv(f\"{dataset}_int2.csv\", header=None)\n",
    "ints.columns = [\"user\", \"item\", \"t1\", \"t2\"]\n",
    "num_items = len(pd.unique(ints['item']))\n",
    "counter = Counter()\n",
    "ints.groupby('user').apply(lambda x: counter.update(list(zip(x['item'], x['item'].loc[1:]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53d7d10-a65f-458f-a28a-7a9660e9d9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain low dimensional vector via randomized svd\n",
    "rows = [x[0] for x in counter.keys()]\n",
    "cols = [x[1] for x in counter.keys()]\n",
    "vals = list(counter.values())\n",
    "finalrows = rows + cols\n",
    "finalcols = rows + cols\n",
    "vals = vals + vals\n",
    "\n",
    "csr = csr_array((vals, (finalrows, finalcols)), shape = (num_items, num_items))\n",
    "norm_csr = normalize(csr)\n",
    "random_state = 2023\n",
    "u, s, v = randomized_svd(norm_csr, n_components=50, n_oversamples=50, random_state=random_state)\n",
    "\n",
    "final = np.zeros(((u*s).shape[0]+1, (u*s).shape[1]))\n",
    "final[0, :] = 0\n",
    "final[1:, :] = (u*s)\n",
    "np.savetxt(f\"{dataset}_copca.txt\", final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:opence-v1.7.2]",
   "language": "python",
   "name": "conda-env-opence-v1.7.2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
